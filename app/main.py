import uvicorn
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1. Initialize the FastAPI App
app = FastAPI(title="Azure Alert Agent (Simple)")

# 2. Setup the Local LLM (Ollama)
# We use the model you selected in the plan: MiniCPM-V or Qwen
llm = ChatOllama(
    model="qwen3-vl:4b",
    temperature=0,  # Keep it deterministic for alerts
)

# 3. Define the Chain
# We create a simple prompt -> LLM -> String parser pipeline
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful Tier 1 Support Agent for Azure."),
        ("user", "{input}"),
    ]
)
chain = prompt | llm | StrOutputParser()


# 4. Define Input Data Model
class ChatRequest(BaseModel):
    message: str


# 5. Standard Endpoint (Waits for full response)
@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    """
    Simple endpoint that waits for the LLM to finish generating
    the entire response before sending it back.
    """
    response = await chain.ainvoke({"input": request.message})
    return {"response": response}


# 6. Streaming Endpoint (Real-time chunks)
@app.post("/stream")
async def stream_endpoint(request: ChatRequest):
    """
    Advanced endpoint that streams the response token-by-token.
    This prevents the webhook from timing out on long generations.
    """

    async def generate():
        # 'astream' yields chunks as they are generated by Ollama
        async for chunk in chain.astream({"input": request.message}):
            yield chunk

    return StreamingResponse(generate(), media_type="text/event-stream")


# 7. Run the Server
if __name__ == "__main__":
    # Run with: python main.py
    uvicorn.run(app, host="0.0.0.0", port=8000)
