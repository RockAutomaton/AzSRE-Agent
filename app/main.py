# Load environment variables from .env file first
from dotenv import load_dotenv
load_dotenv()

import logging
import traceback
import uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from app.schemas import AzureWebhookPayload
from app.graph.workflow import build_graph
from app.graph.state import AgentState

# Configure logging
logger = logging.getLogger(__name__)

# 1. Initialize the FastAPI App
app = FastAPI(title="Azure Alert Agent")

# 2. Setup the Local LLM (Ollama)
# We use the model you selected in the plan: MiniCPM-V or Qwen
llm = ChatOllama(
    model="gemma3:27b",
    temperature=0,  # Keep it deterministic for alerts
)

# 3. Initialize the workflow graph
try:
    graph = build_graph()
except Exception as e:
    logger.error(
        f"Failed to initialize workflow graph: {str(e)}\n"
        f"Traceback:\n{traceback.format_exc()}",
        exc_info=True
    )
    graph = None

# 4. Define the Chain
# We create a simple prompt -> LLM -> String parser pipeline
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful Tier 1 Support Agent for Azure."),
        ("user", "{input}"),
    ]
)
chain = prompt | llm | StrOutputParser()


# 5. Define Input Data Model
class ChatRequest(BaseModel):
    message: str


# 6. Azure Webhook Endpoint
@app.post("/webhook/azure")
async def azure_webhook(payload: AzureWebhookPayload):
    """
    Receives Azure Monitor alerts and processes them through the agent workflow.
    """
    # Verify graph is initialized before any graph-dependent logic
    if graph is None:
        logger.error("Workflow graph not initialized - cannot process alert")
        raise HTTPException(
            status_code=503,
            detail="workflow graph not initialized"
        )
    
    # Initialize state
    initial_state: AgentState = {
        "alert_data": payload.data,
        "investigation_steps": [],
        "final_report": None,
        "classification": None,
    }
    
    # Run the workflow
    final_state = await graph.ainvoke(initial_state)
    
    # Return the results
    return {
        "classification": final_state.get("classification", "UNKNOWN"),
        "report": final_state.get("final_report", "No report generated"),
        "steps": final_state.get("investigation_steps", []),
    }


# 7. Standard Endpoint (Waits for full response)
@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    """
    Simple endpoint that waits for the LLM to finish generating
    the entire response before sending it back.
    """
    response = await chain.ainvoke({"input": request.message})
    return {"response": response}


# 8. Streaming Endpoint (Real-time chunks)
@app.post("/stream")
async def stream_endpoint(request: ChatRequest):
    """
    Advanced endpoint that streams the response token-by-token.
    This prevents the webhook from timing out on long generations.
    """

    async def generate():
        # 'astream' yields chunks as they are generated by Ollama
        async for chunk in chain.astream({"input": request.message}):
            yield chunk

    return StreamingResponse(generate(), media_type="text/event-stream")


# 9. Run the Server
if __name__ == "__main__":
    # Run with: python main.py
    uvicorn.run(app, host="0.0.0.0", port=8000)
